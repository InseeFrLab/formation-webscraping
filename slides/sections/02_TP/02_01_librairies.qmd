## Les [librairies principales](https://www.projectpro.io/article/python-libraries-for-web-scraping/625#mcetoc_1gb5hj7o81i) sur `Python`

<br>

Plusieurs librairies permettent de faire du scraping en `Python`. Les principales sont :

- [Requests](https://requests.readthedocs.io/en/latest/)
- [Urllib](https://docs.python.org/fr/3/library/urllib.html)
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium](https://selenium-python.readthedocs.io/)
- [Scrapy](https://scrapy.org/)
- [lxml](https://lxml.de/)


## Extraire les codes sources HTML

#### `Requests` et `Urllib`

<br>

- Rôle :
    + Créer des requêtes `HTTP` pour récupérer le contenu des pages web
    + Des _headers_ peuvent être ajoutés aux requêtes pour s'identifier auprès du site

. . .

- `Requests` un peu plus simpliste que `Urllib`


## Extraire les codes sources HTML {.smaller}

#### Bonne pratique

- Lorsque l'on scrape plusieurs pages d'un même site, il vaut mieux [__ouvrir une seule session__]{.blue2} sur le site et faire les requêtes à partir de là que réouvrir une nouvelle session à chaque requête.
    + Si l'on scrape plusieurs sites différents, on ouvre __une session par site distinct__.

. . .

- Cela permet notamment d'[__éviter la saturation des serveurs__]{.blue2} en __limitant le nombre de ports utilisés__ lorsque de larges volumes sont scrapés.
    + Cela __réduit également le délai des requêtes successives__, puisqu'il n'y a pas à ouvrir une nouvelle session à chaque requête.

. . .

::: {.callout-tip}
## Exemple avec `requests`

```python

sessions = {}
for url in sites_distincts:
    sessions[url] = requests.Session()

for url in sites_distincts:
    for page in pages[url]:
        request_text = sessions[url].get(url+page, params=params).text
```
:::


## Analyser les codes _HTML_ : `BeautifulSoup`

- Permet de formater des documents `HTML` ou `XML` en une structure arborescente
- Permet ensuite de naviguer dans la structure pour scraper le contenu désiré
- Très intuitif à utiliser

<!-- Ajouter une slide sur les fonctions de BeautifulSoup, comme find et findAll -->

::: {.callout-tip}
## Remarque

La librairie se trouve généralement sous le nom de `bs4`.
:::

## Analyser les codes _HTML_ et _XML_ : `lxml`

- Permet de formater des documents `HTML` ou `XML` en une structure arborescente

- Permet de naviguer dans l'arborescence des balises lorsque celles-ci ne sont pas clairement identifiées
    + Si par exemple il n'y a pas d'_id_ ou de _class_

- Repose notamment sur `XPath`, permettant d'accéder à des balises enfouies profondément dans l'arborescence

- Particulièrement utile pour des pages avec de nombreux éléments emboîtés peu identifiés


## Interagir avec une page : `Selenium`

<br>

- Les librairies précédentes ne permettent pas d'interagir avec une page utilisant `Javascript`
    - Ex: Appuyer sur un bouton, rentrer ses identifiants...

. . .

- `Selenium` permet de simuler un navigateur (aussi appelé [headless browser](https://en.wikipedia.org/wiki/Headless_browser)) puis d'interagir avec les éléments `JavaScript` du site "comme un utilisateur"


## Un outil plus complet : `Scrapy`

<br>

- Un outil plus complet mais aussi plus complexe
- Très utile pour les opérations de web crawling
- Davantage recommandé pour de gros projets
- Un tutoriel des [Carpentries](https://librarycarpentry.org/lc-webscraping/04-scrapy/index.html)


## Quelques librairies sur `R`

- [polite](https://cran.r-project.org/web/packages/polite/index.html)
    - Particulièrement orienté scraping éthique

. . .

- [rvest](https://thinkr.fr/rvest/)
    - Librairie la plus utilisée et donc documentée

. . .

- D'autres librairies : [RCrawler](https://cran.r-project.org/web/packages/Rcrawler/Rcrawler.pdf), [RSelenium](https://cran.r-project.org/web/packages/RSelenium/index.html)

. . .

- Pour manipuler des headless browsers à très bas niveau, voir [Chromote](https://github.com/rstudio/chromote)
    + Aussi disponible sur Python [ici](https://pypi.org/project/chromote/)


## Quelques librairies en `Java`

<br>

- `Java` est également beaucoup utilisé dans le domaine du web scraping

<br>

- Documentation extensive disponible sur Internet
